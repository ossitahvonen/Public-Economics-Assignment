---
title: "Public Economics Assignment"
author: "Miia Janatuinen, Ossi Tahvonen"
date: "23/03/2020"
output: pdf_document
---


###PART A 
1. Read the macro study by FÃ¶lster and Henrekson (2001)
A) Summarize the main econometric approach and the findings in the study.

The authors use combined cross-section time series regressions with 5-year periods. They exploit within country variation to increase the efficiency. They use controls for periods and countries to get rid of short term endogeneity problems (caused by business cycles) and to allow for country specific production functions. They also report regressions using 2SLS (i.e. instrumental variables) to control for business cycles. The problem of selection on the OECD countries is controlled by running regressions with additional data set of more countries. The researchers correct for heteroscedasticity between countries. 

The paper finds significant negative effects between government expenditure and economic growth. A 10% increase in expenditure ratio is associated with a 0.7-0.8 % lower growth. There are no significant effects for taxes found. These effects persist in first differences regressions, using lagged variables as instruments and with additional country data.

The robustness tests show that the effect of government expenditure is more robust than the effect of the taxes. Neither is robust by the strict definition of the extreme bounds analysis.

B) How does the empirical strategy in the paper differ from the micro studies discussed in the class? How credible it is in comparison to studies presented at the course?

The research question is a sense bigger and uses aggregate variables. The amount of possible endogeneity/omitted variable problems and level of possible extrapolation hugely differs from the micro level studies. In many micro-level studies, the identification strategy is based on some kind of (quasi) randomized controllized trial -type of design but this is obviously not possible on macro level. The results of the reported checks are encouraging, as they mostly show no problems, but it is possible that there still are persisting problems in making a causal intrepretation of the estimates (and this is not what the paper suggests, the authors even admit that "In general, it is hardly possible to solve all econometric problems.").



C) How would you comment on the impact of the size of the public sector on economic growth on the based on its results?

The question overall is important and answering it, keeping in mind the restrictions of econometrics, is therefore an important task.

It seems that there is, if the econometric approach is considered viable, a negative impact between the size of the public sector and the economic growth.  As noted before, this is questionable. The robustness tests show that the result can be extended to differing combinations of controlling variables. The effect seems to be somewhat result of chages in savings. This lends credibility to the result. Of course, as the title of the paper suggests, the results are only from the data in rich countries, and the result can not be extrapolated to developing countries.

There is also need to note that the results of this research only concern the sample of relatively high-income countries. If one were to exrapolate the results to some lower income level countries, it could result in a problematic interpretation. The question can therefore be answered only on behalf the countries that represent the sample. In addition, the paper does not concern the structure of the public expenditure. The result does not therefore mean that all public spending is bad for economomic growth and some expenditure might be beneficial (for instance government actions in coronavirus situation). Moreover, a relevant question for policy is also how the growth is distributed and how this affects well-being of people, and this is something the result does not consider.





2. Read the evaluation of the British Working Families' Tax Credit by Blundell et al. (2005).

A) Summarize the main econometric approach and the findings in the study.

Blundell et al. use a probit difference-in-difference model to study the extensive margin labour market effects of the Working Family Tax Credit (WFTC) reform in the UK. The reform affected only families with children, and in order to identify its effects, families with children are considered as a treatment group and similar families without children as the control group. The control group is assumed to reflect the other underlying effects, and hence the changes in the control group's employment are removed from the changes of the treatment group's employment for identification. The treatment variable equals 1 if the observation takes place after the reform and the family has children at the time, and 0 otherwise.  In order to address the average differences between groups, some observable individual and macro-level conditions, , namely the number of children, the age of the youngest child, partner's employment status, general economic conditions, seasonal controls, age and education level, are controlled for. In addition, the phase-in and adjustment periods are dropped from the sample. There were other reforms that affected parents during the sample period, so the effects of WFTC cannot be identified from these other reforms. 

The effects are identified for single mothers, single fathers, mothers in couples and fathers in couples. The estimated results show a significant positive treatment effect of 3.6 ppt on single mothers' employment and the effect was stronger with more and younger children. For all mothers in couples, the treatment effect was insignificant (+0.4ppt). There was however a significant effect of 2,6 ppt for those mothers whose partners were not employed. For single fathers, there was a positive and significant effect of 4.6ppt. For the fathers in couples, the treatment effect was statistically significant -0.5ppt. The effect for fathers in couples was stronger when their partners were working. There is some evidence on the effects decreasing over time and heterogenous effects for couples with young children. The robustness checks indicate the existence of additional underlying trends for couples that are not controlled for. In total, the estimated effects correspond 25,000-59,000 new workers, from which the single mothers' share was around 60,000. 

B) Compare the approach to the US studies on the EITC discussed in the class.

Both studies study the extensive margin labour market responses of tax reforms with a similar ex-post difference-in-difference approach. The tax reforms in both countries affected families with children and both studies use families with similar characteristics but without children as control groups. However, the US study only studies single mothers, while the UK study also includes fathers and couples. 
	
The main difference between the two studies is that the US study studies multiple tax reforms over a long time period and its analysis is hence more extensive. In addition, the control and treatment variables used in the UK paper are similar in the US paper, but the US study also includes more extensive analysis of the confounding factors. While the UK study examines the employment trends of the treatment and control groups close to the reform, the US study also analyses the long-term employment trends of the groups. Finally, the US study considers multiple extensive margin estimates  in addition to the employment status that was the output variable in the UK WFTC study.

C) How would you comment on the credibility of the research design? Are there significant threats to its identification strategy?

The difference-in-difference identification strategy assumes that the control and the treatment groups experience common time effects and that there are no composition changes within the groups. Besides single women, the other treatment and control groups experienced very similar employment trend before and after the reform, so the common trend assumption seems credible. However, as was also noted in the paper, the employment trend of the single mothers had already started to converge before the reform took place, so this could present one threat for the identification strategy. In addition, the employment levels of the individuals without children were very high in general, so the extent to which they can respond to good economic conditions might not fully correspond the real macro effects the treatment group experienced during the reform. 

Another concern for the identification is the presence of other reforms. As was stated in the paper, also other contemporaneous reforms affected parents in addition to the WFTC. Therefore, the common time trend assumption is not satisfied in this respect and the results show the combined effects of all these reforms. The authors conclude that this results in underestimation of the WFTC effects for single mothers and that there is uncertainty about the direction of the bias for other groups.

The control variables in the estimation address the possibility of average differences between the groups. Robustness checks show evidence on the satisfaction of the identification assumptions for single parents. For the couples, the robustness checks indicate some underlying trends that were not controlled for. In summary, the identification strategy has some weaknesses, notably for parents in couples, but if these are taken into account when interpreting the results, the research design is rather credible. 

###PART B

1. Read the article.

2. Open data and provide summary statistics similar to those in table 1. (2p.)

We note here that the data provided in the ".dta" format and the data in ".xls" format are different and provide differing results. We were able to obtain the correct regression results using ".dta" -file so we use it throughout the exercise. The main problem seems to be the totrob-variable, which gets values between 0 and 1 in the "dta" -file but is rounded up or down in the ".xls" -file. The picture drawn in part 4 is also different with both data, and is more similar to the picture in the Assignment session slides.



```{r, message=FALSE, warning=FALSE}
#packages
library(MASS)
#tibble etc
library(tidyverse)
library(dplyr)
#tables
library(xtable)
#plots
library(ggplot2)
library(stargazer)
#statadata
library(foreign)
#robust SE:s
library(lmtest)
library(sandwich)
#fixed effects
library(lfe)

#data
data <- read.dta("MonthlyPanel.dta")

#new dummy that has 0 if no jewish institution in same or neighbouring block
data$jewin <- as.numeric(data$institu1|data$institu3)

#table
data2 <- as.tibble(data)
data2 <- data %>% dplyr::select(distanci, edpub, estserv, banco,totrob, jewin)
table <- cbind(apply(data2[data2$jewin==0,],2,mean),apply(data2[data2$jewin==0,],2,sd),
               apply(data2[data2$jewin==1,],2,mean), 
               apply(data2[data2$jewin==1,],2,sd))[1:5,]
table <- cbind(table,table[,1]-table[,3])

colnames(table) <- c("Census tracts without Jewish institutions (A)",
                     "SD(A)", "Census tracts with Jewish institutions (B)", "SD(B)",
                     "Difference (C)=(A)-(B)")
table1 <- xtable(table, caption="DEMOGRAPHIC CHARACTERISTICS OF CONTROL AND TREATMENT AREAS")

```

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrr}
  \hline
 & \begin{tabular}{@{}l@{}} Census tracts without\\ Jewish institutions (A)\\\end{tabular} & SD(A) & \begin{tabular}{@{}l@{}} Census tracts with\\ Jewish institutions (B)\\\end{tabular}& SD(B) & Difference (C)=(A)-(B) & p \\ 
  \hline
distanci & 3.66 & 1.83 & 0.81 & 0.39 & 2.84 & < 2.2e-16\\ 
  edpub & 0.03 & 0.17 & 0.05 & 0.21 & -0.02 & 0.0003367\\ 
  estserv & 0.02 & 0.15 & 0.02 & 0.12 & 0.01 & 0.02565\\ 
  banco & 0.08 & 0.27 & 0.08 & 0.26 & 0.00 & 0.5485\\ 
  totrob & 0.08 & 0.23 & 0.08 & 0.23 & 0.01 & 0.7718\\ 
   \hline
\end{tabular}
\caption{DEMOGRAPHIC CHARACTERISTICS OF CONTROL AND TREATMENT AREAS} 
\end{table}

The values reported in the column 6 are p-values from Welch two-sample t-test for equal means with 2 sided alternative hypothesis. A small p-value therefore means that the null can be rejected and the difference is unlikely to be equal to zero. For the variable totrob we only test values before the changes in surveillance policy. We did not find any code that would calculate the standard deviation for the difference directly and our manual calculations failed to replicate the numbers in the original paper.


3. Replicate table 3. (3p.)

The authors use mostly Hubert-White standard errors. In table 5, they also examine the clustering in standard errors. The clustering seems to have no effect. As shown in the exercise session, we use clustered standard errors in regressions A-C and E and robust standard errors in regression D. The specifications of functions in R differ between packages and are different from Stata, so the results we obtain are different but accurate up to 4 decimals.


In the data set, institu3 variable is 1 also when there is a protected institution in the same block (i.e. when institut1 = 1). Creating new institu3 variable that equals 1 only when institu1 is 0:

```{r}
data$institu3_neww <- data$institu3-data$institu1

```

Creating new indicator variables for two-block distance from the nearest protected institution (twoblock) and for time after the terrorist attack (postt):
```{r}
data$twoblock <- ifelse(data$distanci==2,1,0)

data$postt <- ifelse(data$mes>7 & data$mes<70,1,0)
```

\textbf{Column A:} The difference-in-difference estimator is 
$$Car \; Theft_{it} = \alpha_0 \; Same \; Block \; Police_{it} +M_t + F_i +\varepsilon_{it}$$
where $Car \; Theft_{it}$ are the number of car thefts in block i for month i, $Same \; Block \; Police_{it}$ is the interaction term between institu1 and postt (=the treatment variable), $M_t$ are the month fixed effects, $F_t$ are block fixed effects and $\varepsilon_{it}$ is the error term. Since block fixed effects are used, we use clustered standard errors. 

Creating the treatment variables and dropping months 72 and 73:

```{r}
data$SameBlock <- data$institu1*data$postt
data$OneBlock <- data$institu3_neww*data$postt
data$TwoBlock <- data$twoblock*data$postt
data <- data[data$mes < 70,]
```

Fixed effect regression (mes and observ as the fixed effects, clustering wrt. observ):

```{r}
m3a <- felm(data=data, totrob ~ SameBlock | mes + observ | 0 | observ)

```

\textbf{Column B:} The difference-in-difference estimator is like in A, but now also includes a treatment variable for one-block distance (One-Block Police = institu3_neww*postt):
$$Car \; Theft_{it} = \alpha_0 \; Same \; Block \; Police_{it} + \alpha_1 One \; Block \; Police_{it} +M_t + F_i +\varepsilon_{it}$$
```{r}
m3b <- felm(data=data, totrob ~ SameBlock + OneBlock | mes + observ | 0 | observ)
```


\textbf{Column C:} Like in B, but adding a treatment variable for Two-Block Police (=twoblock*postt):
$$Car \; Theft_{it} = \alpha_0 \; Same \; Block \; Police_{it} + \alpha_1 One \; Block \; Police_{it} + \alpha_2 Two\; Block\; Police_{it} + M_t + F_i +\varepsilon_{it}$$
```{r}
m3c <- felm(data=data, totrob ~ SameBlock + OneBlock + TwoBlock | mes + observ | 0 | observ)

```

\textbf{Column D}: The cross section regression uses data only from after the terrorist attack and doesn't include block-fixed effects:
$$Car \; Theft_{it} = \beta_0 + \alpha_0 \; Same \; Block \; Police_{it} + \alpha_1 One \; Block \; Police_{it} + \alpha_2 Two\; Block\; Police_{it} + M_t +\varepsilon_{it}$$
Now $Same \; Block \; Police_{it}, \; One \; Block \; Police_{it}$ and $Two\; Block\; Police_{it}$ have not been interacted with the post variable, since data is only from when postt=1. We use robust standard errors. 

```{r}
data_post <- data[data$mes>7,]
data_post$SameBlock <- data_post$institu1
data_post$OneBlock <- data_post$institu3_neww
data_post$TwoBlock <- data_post$twoblock
model3d <- lm(data = data_post, formula = totrob ~ SameBlock + OneBlock + TwoBlock +
                as.factor(mes))

```

Robust standard errors: 
```{r}
coeftest(model3d, vcov= vcovHC(model3d, "HC1"))
```

\textbf{Column E:} The time-series regression only uses data from the Jewish blocks (up to two block distance): 
$$Car \; Theft_{it} = \beta_0 + \alpha_0 \; Same \; Block \; Police_{it} + \alpha_1 One \; Block \; Police_{it} + \alpha_2 Two\; Block\; Police_{it} + F_i +\varepsilon_{it}$$
Also the different lengths of the months are taken into account in the new totrob2 variable: 

```{r}
data$totrobc <- ifelse(data$mes==7, data$totrob*(30/17), data$totrob)
data$totrob2 <- ifelse(data$mes==5|data$mes==8|data$mes==10|data$mes==12,
                        data$totrob*(30/31), data$totrobc)
```

Regression using clustered standard errors: 
```{r}
data_close <- data[data$distanci<3,]
m3e <- felm(data = data_close, totrob2 ~ SameBlock+OneBlock+TwoBlock | observ | 0 | observ) 

```

Collecting the results: 

```{r, message=FALSE, warning=FALSE, results=FALSE}
stargazer(m3a,m3b, m3c, model3d, m3e, column.separate=c(3,1,1), 
          column.labels=c("Difference-in-difference","Cross section"," Time series"),
          dep.var.labels.include = FALSE, dep.var.caption = "", model.names = FALSE,
          summary.stat = c("sd"),  title = "THE EFFECT OF POLICE PRESENCE ON CAR THEFT",
          keep = c("SameBlock", "OneBlock","TwoBlock"), label = "3",
          keep.stat = c("n", "rsq") )
```

(fixing the robust standard errors for column D by hand)

\begin{table}[!htbp] \centering 
  \caption{THE EFFECT OF POLICE PRESENCE ON CAR THEFT} 
  \label{3} 
\begin{tabular}{@{\extracolsep{5pt}}lccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{3}{c}{Difference-in-difference} & Cross section &  Time series \\ 
\\[-1.8ex] & (1) & (2) & (3) & (4) & (5)\\ 
\hline \\[-1.8ex] 
 SameBlock & $-$0.078$^{***}$ & $-$0.080$^{***}$ & $-$0.081$^{***}$ & $-$0.073$^{***}$ & $-$0.058$^{**}$ \\ 
  & (0.023) & (0.024) & (0.024) & (0.011) & (0.023) \\ 
  & & & & & \\ 
 OneBlock &  & $-$0.013 & $-$0.014 & $-$0.012 & $-$0.00005 \\ 
  &  & (0.015) & (0.015) & (0.011) & (0.015) \\ 
  & & & & & \\ 
 TwoBlock &  &  & $-$0.002 & $-$0.003 & 0.017 \\ 
  &  &  & (0.012) & (0.009) & (0.011) \\ 
  & & & & & \\ 
\hline \\[-1.8ex] 
Observations & 7,884 & 7,884 & 7,884 & 4,380 & 3,816 \\ 
R$^{2}$ & 0.198 & 0.198 & 0.198 & 0.004 & 0.189 \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{5}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

The estimates in this regression have been rounded and the precise summaries can be found in the Appendix.


4. Plot a figure of choice describing the DID-effects. (2p.)

```{r, results=F}
#calculate means
datamean_treat <- data[data$institu1==1,] %>% as.tibble() %>%
  group_by(mes) %>%
  summarise_at(vars(totrob),
               list(name2 = mean, sdev2 = sd))

datamean_contr <- data[data$institu1==0,] %>% as.tibble() %>%
  group_by(mes) %>%
  summarise_at(vars(totrob),
               list(name2 = mean, sdev2 = sd))
datamean_treat

datamean <- cbind(as.data.frame(datamean_contr),as.data.frame(datamean_treat[,2:3]))[1:9,]
datamean <-  cbind(datamean, datamean_contr[1:9,2] - datamean_treat[1:9,2])
#the correct values for the standard errors are sd/sqrt(n)

n_contr <- nrow(data[data$institu1==0,])
n_treat <- nrow(data[data$institu1==1,])

colnames(datamean) <- c("mes","name2", "sdev2", "name", "sdev", "dif")

#set colors
colors1 <- c("Treatment Group" = "red", "Control Group" = "blue")
#draw
p1 <- ggplot(data = datamean, aes(x=mes)) +
  geom_point(aes(x=mes, y = name, color = "Treatment Group")) + geom_line(aes(x=mes, y = name, color = "Treatment Group")) +
  geom_point(aes(x = mes, y = name2, colour = "Control Group")) + 
  geom_line(data = datamean, aes(x = mes, y = name2, color = "Control Group")) +
  geom_errorbar(data = datamean, aes(ymin = name - 1.96*sdev/sqrt(n_treat), ymax = name + 1.96*sdev/sqrt(n_treat)), width=.2, position=position_dodge(.9), colour = "red") +
  geom_errorbar(data = datamean, aes(ymin = name2 - 1.96*sdev2/sqrt(n_contr), ymax = name2 + 1.96*sdev2/sqrt(n_contr)), width=.2, position=position_dodge(.9), colour = "blue") +
  geom_vline(xintercept = 7.5, linetype = 2) + 
  xlab("Months") + ylab("Mean crime") +
  theme(legend.position="bottom") +
  labs(x = "Months",
       y = "Mean Car Theft per month, 95% CI",
       color = "Legend") +
  scale_color_manual(values = colors1)

```

```{r}
p1
```


5. Discuss the results. (3p.)
a. What do the estimation results in table 3 tell?

There are statistically significant effects for the increased police presence on the same block across all specifications. This together with the consideration of the identification scheme, provides evidence that the changes in surveillance policy did indeed decrease the amount of crime in the neighborhoods affected.

Instead, the coefficients for the other police presence variables are not statistically significantly different from zero in any regressions. This shows that the effect of the  increased surveillance was quite local, but also that there seems to not be any displacement of crime to these neighborhoods, which could be considered next targets as the criminals assumably prefer closeby targets.

Similar conlusions can be reached with different regressions, ie. time series and cross-sectional data. The times series regression considers only the blocks with Jewish institutions and finds effects correlating with the survellance policy. This shows that results are robust, because nothing remarkably different arises from these regressions, since the coefficients are almost similar in all cases. 

The authors also argue that the effects observed are just the deterrence effects, ie. effects following from criminals being scared to commit crimes in the presence of police officers, and not effects from incapacitation of criminals. This argument is based on the fact that the policemen were essentially standing in front of buildings and not tasked to apprehend car-stealing criminals.

b. What are the potential concerns in the research set up?

Using differences in differences estimation, the most important threat to estimation is violation if the parallel trends assumption. This is required for the control group to provide a credible counterfactual to treatment. There is no statistical test for this assumption. It can be assessed visually from graphs like Picture 1. The picture however only shows the means and not the controls, namely block and month dummies, used in regressions. The trends seem to be somewhat parellel before the treatment. Table 4 of the paper also presents estimates for different time periods to check for this assumption, and finds that there are effects even if the treatment is started later.

It also seems that the increase surveillance was randomly determined in the sense that the areas with and without Jewish institutions are remarkable similar, as can be seen from Table 1. So the design seems a credible solution to the endogeneity problem, ie. more police officers in areas with already more crime.

The authors also explore the possibility that there were other simultaneous treatments in addition to the increase in police presence, such as parking restrictions. They present evidence that this alone can not explain the change in car thefts. They also examine the chance that people (drivers or criminals) adjusted their behaviour because of fear and this caused the reduction in thefts instead of the police presence. They find no evidence of short effects like this, which still leaves the possibility of the effect being longer than the data. 

Of course,if there are problems in the data, a perfect research design will still fail. The authors consider some problems in the data, ie. reporting of crime adn effectiveness of police preventing car theft while securing other locations, and present evidence of why these do not cause problems for the research.


c. What do we learn from the paper? Discuss the internal and external validity of the study.

The internal validity is addressed in the previous question and somewhat in the other parts of the exercise. We find it to be adequate.

On the external validity, some questions can be expressed. The authors examine effects of car price, time of day and day of the week. There are still effects, which lends credibility to idea that the results could be extrapolated. However, the neighborhoods considered were high-middle income areas and represents a small part of the city, with most thefts concentrating on low-income areas.

Important question regarding the validity is the chance that crime simply switched neighborhoods. There are no significant effects on the neighboring blocks, where the crime could have moved, as shown in table 3. Other than this, the data does not allow us to learn about the displacement of crime. This is a key threat to using the results as a guideline to public policy and investments in police forces.



## References

## Appendix
 
 
###1.
Summaries of the regressions in Table 3. 


```{r}
summary(m3a)
summary(m3b)
summary(m3c)
summary(model3d)
summary(m3e)
```

###2. 

```{r, include=FALSE}
data <- read.csv2("MonthlyPanel.csv", header = T)
#data <- read.dta("MonthlyPanel.dta")


########################
#2. Open data and provide summary statistics similar to those in table 1.

#institu1 = dummy for jewish institution
#institu3 = dummy for jewish institution 1 block away

#new dummy that has 0 if no jewish institution in same or neighbouring block
jewin <- as.numeric(data$institu1|data$institu3)
data <- cbind(data,jewin)
#table
#(hieman hankala tapa tehdÃ¤)
data2 <- as.tibble(data)
data2 <- data %>% dplyr::select(distanci, edpub, estserv, banco,totrob, jewin)
table <- cbind(apply(data2[data2$jewin==0,],2,mean),apply(data2[data2$jewin==1,],2,mean))[1:5,]
table <- cbind(table,table[,1]-table[,2])
colnames(table) <- c("Census tracts without Jewish institutions", "Census tracts with Jewish institutions", "Difference")
table



#new institu3
data$institu3_neww <- data$institu3-data$institu1


#ie. month fixed effect
data$twoblock <- ifelse(data$distanci==2,1,0)

#create the post variable
data$postt <- ifelse(data$mes>7 & data$mes<70,1,0)


datamean_treat <- data[data$institu1==1,] %>% as.tibble() %>%
  group_by(mes) %>%
  summarise_at(vars(totrob),
               list(name2 = mean, sdev2 = sd))

datamean_contr <- data[data$institu1==0,] %>% as.tibble() %>%
  group_by(mes) %>%
  summarise_at(vars(totrob),
               list(name2 = mean, sdev2 = sd))
datamean_treat

datamean <- cbind(as.data.frame(datamean_contr),as.data.frame(datamean_treat[,2:3]))[1:9,]
datamean <-  cbind(datamean, datamean_contr[1:9,2] - datamean_treat[1:9,2])
#the correct values for the standard errors are sd/sqrt(n)

n_contr <- nrow(data[data$institu1==0,])
n_treat <- nrow(data[data$institu1==1,])

colnames(datamean) <- c("mes","name2", "sdev2", "name", "sdev", "dif")


colors1 <- c("Treatment Group" = "red", "Control Group" = "blue")

p1 <- ggplot(data = datamean, aes(x=mes)) +
  geom_point(aes(x=mes, y = name, color = "Treatment Group")) + geom_line(aes(x=mes, y = name, color = "Treatment Group")) +
  geom_point(aes(x = mes, y = name2, colour = "Control Group")) + 
  geom_line(data = datamean, aes(x = mes, y = name2, color = "Control Group")) +
  geom_errorbar(data = datamean, aes(ymin = name - 1.96*sdev/sqrt(n_treat), ymax = name + 1.96*sdev/sqrt(n_treat)), width=.2, position=position_dodge(.9), colour = "red") +
  geom_errorbar(data = datamean, aes(ymin = name2 - 1.96*sdev2/sqrt(n_contr), ymax = name2 + 1.96*sdev2/sqrt(n_contr)), width=.2, position=position_dodge(.9), colour = "blue") +
  geom_vline(xintercept = 7.5, linetype = 2) + 
  xlab("Months") + ylab("Mean crime") +
  theme(legend.position="bottom") +
  labs(x = "Months",
       y = "Mean Crime, 95% CI",
       color = "Legend") +
  scale_color_manual(values = colors1)

```
Figure with the ".xls" -file. Notable there are some differences reflecting the differences in the datasets. The code is similar to the code for the other graph and is not shown.

```{r, echo=FALSE}
p1
```